{\rtf1\ansi\ansicpg1252\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ReadMe f\'fcr rl3DSimulinkModel\
\
Der Hauptcode ist rlEasyModel.m. \
Das vereinfachte Simulink-Modell ist EasyModel.slx.\
\
In dem Ordner \'93savedAgents\'94 steht einen Ordner \'93WPObstWP\'94, der den trainierten Agent von jeweils  3000 Episoden und weitere 4000 Episoden(insgesamt 7000 Episoden). \
\
Der Agent wurde mit den Programmen \'93DDPGAgent.m\'94 und \'93createNetorks.m\'94 erstellt, die eigentlich den Agent von einem Beispiel von Mathworks \'93RL Walker\'94 erstellen. Weil default Einstellung von Agent nicht funktioniert hat und die Anzahl und Aufbau eines neuronalen Netzwerk Erfahrung bezogen ist, ist der Agent von Walker importiert, der 29 Observations und 6 Outputs hat, was sicher genug f\'fcr das EasyModel ist.\
\
URL: https://jp.mathworks.com/help/reinforcement-learning/ug/train-biped-robot-to-walk-using-reinforcement-learning-agents.html\
\
\'93RewardTest.m\'94 visuallisiert die Belohnungsverteilung.}